# Module PR 05: Code Generator

**Module**: `src/code_generator.py`  
**Priority**: P0 (Core - Generates executable Python code)  
**Estimated Effort**: 3-4 days  
**Dependencies**: `01_data_models`, `02_llm_client`, `04_spec_generator`

---

## 1. Module Purpose

The Code Generator transforms a formal `ToolSpec` into executable Python code that:
- Implements the specified functionality using pandas
- Includes proper type hints and docstrings
- Handles errors gracefully
- Follows MCP tool conventions (decorators, JSON footer)
- Is ready for validation and execution

**Key Principle**: Generated code must be production-ready, not just syntactically correct. It should handle edge cases, validate inputs, and provide clear error messages.

---

## 2. Core Components

### 2.1 CodeGenerator Class

```python
class CodeGenerator:
    """Generate Python code from ToolSpec."""
    
    def __init__(self, llm_client: BaseLLMClient, templates_dir: str):
        self.llm = llm_client
        self.templates_dir = Path(templates_dir)
        self.templates = self._load_templates()
    
    def generate(
        self,
        spec: ToolSpec,
        output_path: Optional[Path] = None
    ) -> GeneratedCode:
        """
        Generate Python code from ToolSpec.
        
        Args:
            spec: Tool specification
            output_path: Optional path to write code to
        
        Returns:
            GeneratedCode with code, tests, and metadata
        """
        pass
    
    def _load_templates(self) -> Dict[str, str]:
        """Load code templates from files."""
        pass
    
    def _generate_with_llm(self, spec: ToolSpec) -> str:
        """Use LLM to generate code from spec."""
        pass
    
    def _apply_template(self, spec: ToolSpec) -> str:
        """Apply template-based generation for simple cases."""
        pass
    
    def _add_mcp_decorator(self, code: str, spec: ToolSpec) -> str:
        """Add MCP @tool decorator and metadata."""
        pass
    
    def _generate_tests(self, spec: ToolSpec, code: str) -> str:
        """Generate pytest tests for the tool."""
        pass
    
    def _format_code(self, code: str) -> str:
        """Format code with black and isort."""
        pass
    
    def _validate_syntax(self, code: str) -> None:
        """Check for syntax errors."""
        pass
```

### 2.2 GeneratedCode Model

```python
class GeneratedCode(BaseModel):
    """Container for generated code and metadata."""
    
    code: str  # Main tool code
    tests: str  # Pytest test code
    imports: List[str]  # Required imports
    dependencies: List[str]  # pip packages
    metadata: Dict[str, Any]  # Generation metadata
    
    def save(self, output_dir: Path) -> Tuple[Path, Path]:
        """Save code and tests to files."""
        pass
```

---

## 3. Code Templates

### 3.1 MCP Tool Template

```python
MCP_TOOL_TEMPLATE = '''
"""
{description}

Auto-generated by MCP Tool Generator.
"""

from typing import Any, Dict, Optional
import pandas as pd
from fastmcp import FastMCP

mcp = FastMCP("data_analysis_tools")


@mcp.tool()
def {function_name}({parameters}) -> Dict[str, Any]:
    """
    {docstring}
    
    Args:
{arg_docs}
    
    Returns:
        Dict containing:
        - result: Transformed DataFrame
        - summary: Human-readable summary
        - metadata: Row count and column info
    
    Raises:
{raises_docs}
    """
    # Input validation
{validation_code}
    
    try:
        # Main logic
{main_logic}
        
        # Generate summary
{summary_generation}
        
        return {{
            "result": result.to_dict(orient="records"),
            "summary": summary,
            "metadata": {{
                "row_count": len(result),
                "columns": list(result.columns)
            }}
        }}
    
    except Exception as e:
        return {{
            "error": str(e),
            "error_type": type(e).__name__
        }}


# MCP Tool Metadata (JSON footer)
# {{
#   "tool_name": "{function_name}",
#   "description": "{description}",
#   "version": "1.0.0",
#   "generated_at": "{timestamp}"
# }}
'''
```

### 3.2 Test Template

```python
TEST_TEMPLATE = '''
"""
Tests for {function_name}.

Auto-generated by MCP Tool Generator.
"""

import pytest
import pandas as pd
from {module_name} import {function_name}


def test_{function_name}_basic():
    """Test basic functionality."""
{test_basic_code}


def test_{function_name}_empty_dataframe():
    """Test with empty DataFrame."""
{test_empty_code}


def test_{function_name}_invalid_column():
    """Test error handling for invalid column."""
{test_invalid_code}


def test_{function_name}_output_structure():
    """Test output structure matches spec."""
{test_output_structure_code}
'''
```

---

## 4. Implementation

### 4.1 Main Generation Flow

```python
def generate(self, spec: ToolSpec, output_path: Optional[Path] = None) -> GeneratedCode:
    """Generate complete tool code."""
    
    # Step 1: Decide generation strategy
    if self._is_simple_operation(spec):
        code = self._apply_template(spec)
    else:
        code = self._generate_with_llm(spec)
    
    # Step 2: Add MCP decorator and metadata
    code = self._add_mcp_decorator(code, spec)
    
    # Step 3: Format code
    code = self._format_code(code)
    
    # Step 4: Validate syntax
    self._validate_syntax(code)
    
    # Step 5: Generate tests
    tests = self._generate_tests(spec, code)
    
    # Step 6: Extract dependencies
    imports = self._extract_imports(code)
    dependencies = self._extract_dependencies(code)
    
    # Create result
    result = GeneratedCode(
        code=code,
        tests=tests,
        imports=imports,
        dependencies=dependencies,
        metadata={
            "tool_name": spec.tool_name,
            "generated_at": datetime.now().isoformat(),
            "generation_method": "template" if self._is_simple_operation(spec) else "llm"
        }
    )
    
    # Optionally save to file
    if output_path:
        result.save(output_path.parent)
    
    return result
```

### 4.2 Template-Based Generation

```python
def _apply_template(self, spec: ToolSpec) -> str:
    """
    Generate code using templates for simple operations.
    
    Handles:
    - Simple group-by with aggregation
    - Column filtering
    - Row filtering
    - Sorting
    """
    operation = self._detect_operation_type(spec)
    
    if operation == "group_by":
        return self._generate_groupby_code(spec)
    elif operation == "filter":
        return self._generate_filter_code(spec)
    elif operation == "sort":
        return self._generate_sort_code(spec)
    else:
        raise ValueError(f"No template for operation: {operation}")


def _generate_groupby_code(self, spec: ToolSpec) -> str:
    """Generate code for group-by operations."""
    
    # Extract parameters from input schema
    params = spec.input_schema.get("properties", {})
    group_cols = [
        p for p in params.keys()
        if p.endswith("_column") and p != "df"
    ]
    
    # Extract metrics
    metrics = params.get("metrics", {}).get("enum", ["count"])
    
    # Build aggregation dict
    if "count" in metrics:
        agg_code = "result = df.groupby(group_columns).size().reset_index(name='count')"
    else:
        agg_dict = {m: m for m in metrics}
        agg_code = f"result = df.groupby(group_columns).agg({agg_dict}).reset_index()"
    
    code = f'''
def {spec.tool_name}(df: pd.DataFrame, {', '.join(f'{gc}: str' for gc in group_cols)}) -> Dict[str, Any]:
    """
    {spec.description}
    """
    # Validate columns exist
    group_columns = [{', '.join(group_cols)}]
    missing = set(group_columns) - set(df.columns)
    if missing:
        raise ValueError(f"Columns not found: {{missing}}")
    
    # Perform groupby
    {agg_code}
    
    # Generate summary
    summary = f"Grouped by {{group_columns}}, found {{len(result)}} groups"
    
    return {{
        "result": result.to_dict(orient="records"),
        "summary": summary,
        "metadata": {{
            "row_count": len(result),
            "columns": list(result.columns)
        }}
    }}
'''
    return code
```

### 4.3 LLM-Based Generation

```python
def _generate_with_llm(self, spec: ToolSpec) -> str:
    """Generate code using LLM for complex operations."""
    
    prompt = self._build_code_generation_prompt(spec)
    
    response = self.llm.generate(
        prompt=prompt,
        temperature=0.4,  # Moderate creativity
        max_tokens=2000
    )
    
    # Extract code block from response
    code = self._extract_code_block(response)
    
    return code


def _build_code_generation_prompt(self, spec: ToolSpec) -> str:
    """Build prompt for LLM code generation."""
    
    return f'''Generate a Python function that implements this specification.

## Function Specification
**Name**: {spec.tool_name}
**Description**: {spec.description}

**Input Schema**:
```json
{json.dumps(spec.input_schema, indent=2)}
```

**Output Schema**:
```json
{json.dumps(spec.output_schema, indent=2)}
```

**Constraints**:
{chr(10).join(f"- {c}" for c in spec.constraints)}

## Requirements
1. Use pandas for data manipulation
2. Add type hints for all parameters
3. Include comprehensive docstring
4. Validate inputs before processing
5. Handle errors gracefully with try/except
6. Return dict with "result", "summary", and "metadata" keys
7. Convert DataFrame to dict with orient="records"
8. Include helpful error messages

## Output Format
Return ONLY the function code, no explanations. Use this structure:

```python
def {spec.tool_name}(df: pd.DataFrame, ...) -> Dict[str, Any]:
    """
    {spec.description}
    
    Args:
        df: Input DataFrame
        ...: Other parameters
    
    Returns:
        Dict with result, summary, and metadata
    """
    # Your implementation here
```

Generate the function:'''
```

### 4.4 MCP Decorator Addition

```python
def _add_mcp_decorator(self, code: str, spec: ToolSpec) -> str:
    """Add MCP @tool decorator and imports."""
    
    # Extract function definition
    func_match = re.search(r'def\s+\w+\([^)]*\)', code)
    if not func_match:
        raise ValueError("Could not find function definition")
    
    # Build complete code with imports and decorator
    full_code = f'''"""
{spec.description}

Auto-generated MCP Tool.
"""

from typing import Any, Dict, Optional
import pandas as pd
from fastmcp import FastMCP

mcp = FastMCP("data_analysis_tools")


@mcp.tool()
{code}


# MCP Tool Metadata (JSON footer)
# {{
#   "tool_name": "{spec.tool_name}",
#   "description": "{spec.description}",
#   "version": "1.0.0",
#   "input_schema": {json.dumps(spec.input_schema)},
#   "output_schema": {json.dumps(spec.output_schema)},
#   "generated_at": "{datetime.now().isoformat()}"
# }}
'''
    
    return full_code
```

### 4.5 Code Formatting

```python
def _format_code(self, code: str) -> str:
    """Format code with black and organize imports with isort."""
    
    try:
        # Format with black
        import black
        code = black.format_str(code, mode=black.FileMode())
        
        # Organize imports with isort
        import isort
        code = isort.code(code)
        
    except Exception as e:
        logger.warning(f"Code formatting failed: {e}")
        # Return unformatted code rather than failing
    
    return code
```

### 4.6 Test Generation

```python
def _generate_tests(self, spec: ToolSpec, code: str) -> str:
    """Generate pytest tests for the tool."""
    
    # Extract function name
    func_name = spec.tool_name
    
    # Build sample input from schema
    sample_input = self._generate_sample_input(spec)
    
    # Generate test code
    tests = f'''"""
Tests for {func_name}.

Auto-generated by MCP Tool Generator.
"""

import pytest
import pandas as pd
from src.tools.staged.{func_name} import {func_name}


def test_{func_name}_basic():
    """Test basic functionality."""
    df = pd.DataFrame({sample_input})
    
    result = {func_name}(df, ...)
    
    assert "result" in result
    assert "summary" in result
    assert "metadata" in result
    assert isinstance(result["result"], list)


def test_{func_name}_empty_dataframe():
    """Test with empty DataFrame."""
    df = pd.DataFrame()
    
    result = {func_name}(df, ...)
    
    # Should handle empty input gracefully
    assert "result" in result or "error" in result


def test_{func_name}_invalid_column():
    """Test error handling for invalid column."""
    df = pd.DataFrame({{"A": [1, 2, 3]}})
    
    result = {func_name}(df, invalid_column="nonexistent")
    
    assert "error" in result
    assert "not found" in result["error"].lower()


def test_{func_name}_output_structure():
    """Test output matches spec."""
    df = pd.DataFrame({sample_input})
    
    result = {func_name}(df, ...)
    
    # Validate against output schema
    assert "result" in result
    assert "metadata" in result
    assert "row_count" in result["metadata"]
    assert "columns" in result["metadata"]
'''
    
    return tests
```

---

## 5. Testing Requirements

### 5.1 Unit Tests

```python
import pytest
from src.code_generator import CodeGenerator
from src.models import ToolSpec

def test_generate_simple_groupby():
    """Test generation of simple group-by code."""
    spec = ToolSpec(
        tool_name="group_by_state",
        description="Group by state and count",
        input_schema={
            "type": "object",
            "properties": {
                "df": {"type": "object"},
                "state_column": {"type": "string", "enum": ["state"]}
            }
        },
        output_schema={
            "type": "object",
            "properties": {
                "result": {"type": "object"}
            }
        },
        constraints=[]
    )
    
    generator = CodeGenerator(MockLLMClient(), "templates")
    result = generator.generate(spec)
    
    assert "def group_by_state" in result.code
    assert "@mcp.tool()" in result.code
    assert "groupby" in result.code
    assert len(result.tests) > 0


def test_code_formatting():
    """Test code formatting with black."""
    ugly_code = "def  foo( x,y ):\n  return x+y"
    
    generator = CodeGenerator(MockLLMClient(), "templates")
    formatted = generator._format_code(ugly_code)
    
    assert "def foo(x, y):" in formatted
    assert "    return x + y" in formatted


def test_syntax_validation():
    """Test syntax error detection."""
    invalid_code = "def foo(:\n  return x"
    
    generator = CodeGenerator(MockLLMClient(), "templates")
    
    with pytest.raises(SyntaxError):
        generator._validate_syntax(invalid_code)


def test_mcp_decorator_addition():
    """Test MCP decorator is added correctly."""
    spec = ToolSpec(
        tool_name="test_tool",
        description="Test",
        input_schema={},
        output_schema={},
        constraints=[]
    )
    
    code = "def test_tool(df):\n    return {}"
    
    generator = CodeGenerator(MockLLMClient(), "templates")
    result = generator._add_mcp_decorator(code, spec)
    
    assert "@mcp.tool()" in result
    assert "from fastmcp import FastMCP" in result
    assert "# MCP Tool Metadata" in result
```

### 5.2 Integration Tests

```python
def test_end_to_end_code_generation():
    """Test complete flow from spec to executable code."""
    spec = ToolSpec(
        tool_name="filter_high_severity",
        description="Filter accidents with severity > 3",
        input_schema={
            "type": "object",
            "properties": {
                "df": {"type": "object"},
                "severity_threshold": {"type": "integer"}
            }
        },
        output_schema={
            "type": "object",
            "properties": {
                "result": {"type": "object"}
            }
        },
        constraints=["severity_threshold must be >= 1"]
    )
    
    llm_client = QwenLLMClient(base_url="http://localhost:8000/v1")
    generator = CodeGenerator(llm_client, "templates")
    
    result = generator.generate(spec)
    
    # Validate syntax
    compile(result.code, "<string>", "exec")
    
    # Validate structure
    assert "@mcp.tool()" in result.code
    assert "def filter_high_severity" in result.code
    assert "# MCP Tool Metadata" in result.code
    
    # Execute generated code
    namespace = {}
    exec(result.code, namespace)
    
    # Test execution
    import pandas as pd
    test_df = pd.DataFrame({
        "severity": [1, 2, 3, 4, 5]
    })
    
    output = namespace["filter_high_severity"](test_df, 3)
    assert "result" in output
    assert len(output["result"]) == 2  # severity 4, 5
```

---

## 6. Configuration

```yaml
code_generation:
  templates_dir: "templates/code"
  llm_temperature: 0.4
  max_tokens: 2000
  
  formatting:
    use_black: true
    black_line_length: 100
    use_isort: true
  
  validation:
    check_syntax: true
    check_type_hints: true
    check_docstrings: true
  
  defaults:
    mcp_server_name: "data_analysis_tools"
    tool_version: "1.0.0"
```

---

## 7. Dependencies

### 7.1 Internal
- `src/models.py` - ToolSpec, GeneratedCode
- `src/llm_client.py` - BaseLLMClient

### 7.2 External
```txt
black>=23.0.0
isort>=5.12.0
ast>=3.11  # Built-in
```

---

## 8. Implementation Checklist

- [ ] Create `CodeGenerator` class
- [ ] Create code templates (MCP tool, tests)
- [ ] Implement template-based generation (group-by, filter, sort)
- [ ] Implement LLM-based generation with prompts
- [ ] Implement `_add_mcp_decorator()` for MCP compliance
- [ ] Implement `_format_code()` with black/isort
- [ ] Implement `_validate_syntax()` with AST parsing
- [ ] Implement `_generate_tests()` for pytest
- [ ] Implement `GeneratedCode` model with save() method
- [ ] Add unit tests (>90% coverage)
- [ ] Add integration tests with real code execution
- [ ] Test with multiple spec types
- [ ] Code review and refactor

---

## 9. Example Usage

```python
from src.code_generator import CodeGenerator
from src.llm_client import QwenLLMClient
from src.models import ToolSpec

# Initialize
llm_client = QwenLLMClient(base_url="http://localhost:8000/v1")
generator = CodeGenerator(
    llm_client=llm_client,
    templates_dir="templates/code"
)

# Create spec
spec = ToolSpec(
    tool_name="group_by_state_and_severity",
    description="Group traffic accidents by state and severity",
    input_schema={
        "type": "object",
        "properties": {
            "df": {"type": "object"},
            "state_column": {"type": "string", "enum": ["state"]},
            "severity_column": {"type": "string", "enum": ["severity"]}
        },
        "required": ["df", "state_column", "severity_column"]
    },
    output_schema={
        "type": "object",
        "properties": {
            "result": {"type": "object"},
            "summary": {"type": "string"}
        }
    },
    constraints=[]
)

# Generate code
result = generator.generate(spec, output_path=Path("src/tools/staged/group_by_state_and_severity.py"))

print(f"Generated {len(result.code)} characters of code")
print(f"Generated {len(result.tests)} characters of tests")
print(f"Dependencies: {result.dependencies}")
```

**Output**: Executable Python file saved to staging directory, ready for validation.

---

**Estimated Lines of Code**: 800-1000  
**Test Coverage Target**: >90%  
**Ready for Implementation**: âœ…
