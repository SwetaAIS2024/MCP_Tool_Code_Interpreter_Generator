# LLM Configuration
llm:
  base_url: "http://localhost:11434/v1"  # Ollama default port
  
  # Model selection by task
  models:
    reasoning: "deepseek-r1:70b"  # For intent extraction and planning
    coding: "qwen2.5-coder:32b"   # For code generation and repair
    default: "qwen2.5-coder:32b"  # Fallback model
  
  # Legacy single model config (fallback)
  model: "qwen2.5-coder:32b"
  temperature: 0.3

# Paths
paths:
  draft_dir: "./tools/draft"
  staged_dir: "./tools/staged"
  active_dir: "./tools/active"
  registry: "./registry/tools.json"
  sandbox_workspace: "./tools/sandbox"

# Validation
validation:
  max_repair_attempts: 3
  sandbox_timeout_seconds: 30

# Sandbox
sandbox:
  mode: "subprocess"  # or "docker"
  timeout_seconds: 30
  memory_limit_mb: 512

# Logging
logging:
  level: "INFO"
  file: "./logs/pipeline.log"
