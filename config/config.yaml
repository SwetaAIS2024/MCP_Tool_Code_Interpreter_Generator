# LLM Configuration
llm:
  base_url: "http://localhost:11434/v1"  # Ollama default port
  
  # Model selection by task
  models:
    reasoning: "deepseek-r1:70b"  # For intent extraction and planning
    coding: "qwen2.5-coder:32b"   # For code generation and repair
  temperature: 0.0

# Paths
paths:
  draft_dir: "./tools/draft"
  staged_dir: "./tools/staged"
  active_dir: "./tools/active"
  registry: "./registry/tools.json"
  sandbox_workspace: "./tools/sandbox"

# Validation
validation:
  max_repair_attempts: 3
  sandbox_timeout_seconds: 30

# Sandbox
sandbox:
  # mode: "subprocess"  # or "docker"
  mode: "docker"
  timeout_seconds: 60  # Increased for Docker container startup
  memory_limit_mb: 512

# Logging
logging:
  level: "INFO"
  file: "./logs/pipeline.log"
